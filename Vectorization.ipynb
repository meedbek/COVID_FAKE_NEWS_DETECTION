{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMKhqUT_PU0f"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RKvkYzhJBh1S"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_134736/3658042922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nk\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYIrpUkuAK18"
   },
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWb8XwL0Bh1V",
    "outputId": "a8deabe2-7249-4a9f-fcd4-f894186bab49"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGQ5EhS1Bh1X"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Data-FakeRealCOVID.xlsx\")\n",
    "#df = pd.read_excel(\"/content/drive/MyDrive/PROJET_IA/Data-FakeRealCOVID.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "9uR7foXZBh1Y",
    "outputId": "a7668a5a-8644-45eb-d078-69c5020c2542"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svD7uZt6PgxM"
   },
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpVR8FtbOXmX"
   },
   "source": [
    "**Harmonization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8MREbtdBh1a"
   },
   "outputs": [],
   "source": [
    "def lowerTweet(tweet):\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "h10LTyP8AK2G",
    "outputId": "84e9287f-7c80-432b-9982-2ce6c63c1773"
   },
   "outputs": [],
   "source": [
    "df.tweet.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "SMwYKRrWAK2H",
    "outputId": "a1e1b995-1e7f-47bd-dbc6-6b090802f09e"
   },
   "outputs": [],
   "source": [
    "lowerTweet(df.tweet.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT1UKNIwAK2J"
   },
   "source": [
    "**Remove URLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4H6l4kg_Bh1d"
   },
   "outputs": [],
   "source": [
    "# In order to remove any URL within a string in Python, you can use this RegEx function :\n",
    "def removeURLs(tweet):\n",
    "        return re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "96VIbAd_AK2L",
    "outputId": "54d12e88-97f2-4fe1-b53b-a8a55e3a1ca7"
   },
   "outputs": [],
   "source": [
    "df.loc[4,'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "jb4ZbzuGBh1g",
    "outputId": "9cbd3834-6b76-4215-fb16-bfe53b4b8c77"
   },
   "outputs": [],
   "source": [
    "removeURLs(df.loc[4,'tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiUWKGfaBh1g"
   },
   "source": [
    "**Remove emojis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "_nMd6A1uBh1h",
    "outputId": "1e47e04d-4db6-4774-939e-ae384643c64f"
   },
   "outputs": [],
   "source": [
    "#i took an example to work on \n",
    "example = df.loc[89,'tweet']\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "nBl6paisAK2O",
    "outputId": "e062d1ec-516b-4685-e35b-4d8441fdbf3d"
   },
   "outputs": [],
   "source": [
    "# Remove all traces of emoji from a text file.\n",
    "\n",
    "'''def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "remove_emoji(df.loc[6416,'tweet'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23hpmeK4Bh1j"
   },
   "outputs": [],
   "source": [
    "#the easiet solution we found is to turn text incode into ascii wich does not include emojis and ignore all other characters that are not defined on ascii encode\n",
    "#we use encode function which turns string into ascii code and ignoring none ascii characters\n",
    "#then decode to make the ascii code a string again\n",
    "def toAscii(tweet):\n",
    "  return  (tweet.encode('ascii',errors='ignore')).decode('ascii') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5fC1-oelAK2Q",
    "outputId": "945009dd-5233-462e-cfe4-2d2f24c3135d"
   },
   "outputs": [],
   "source": [
    "toAscii(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be6crzaPD4po"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhWZSTsaAK2Q"
   },
   "source": [
    "**Remove punctuation & special characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_TEK1ZFAK2Q"
   },
   "outputs": [],
   "source": [
    "#removing all numbers ,ponctuation & special characters such as @ and #\n",
    "#refrence to https://www.kerryr.net/pioneers/ascii3.htm\n",
    "\n",
    "def removeSpecialChar(text):\n",
    "    text = list(text)\n",
    "    \n",
    "    #remove special character & ponctuations & numbers\n",
    "    listOfSpecialChars =  [*range(33,48),*range(58,65),*range(91,97),*range(123,127)]\n",
    "    listOfSpecialChars.remove(39) #dont include ' in the list\n",
    "    \n",
    "    text = [char for char in text if (ord(char) not in listOfSpecialChars and not char.isdigit())] \n",
    "    text = ''.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "F6Xwd6YoAK2R",
    "outputId": "fbeb7075-a46a-4dd0-cbd8-ff6246602bad"
   },
   "outputs": [],
   "source": [
    "#lets test on this example\n",
    "df.loc[264].tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "1IdKq6JTAK2S",
    "outputId": "b2a2bbff-c8a0-49fc-f2ec-619ecc602f70"
   },
   "outputs": [],
   "source": [
    "removeSpecialChar(df.loc[264,'tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVEgu9rVGKbY"
   },
   "source": [
    "**remove apostrophe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opwnYIyVGXJM"
   },
   "outputs": [],
   "source": [
    "def removeApos(text):\n",
    "    text = text.replace(\"'\",'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ZrryvB5ZG_2q",
    "outputId": "aada0e0e-48d6-4ea5-cd5a-e924cafb59b0"
   },
   "outputs": [],
   "source": [
    "#lets test on this example\n",
    "df.loc[264].tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Q98VDfmrG_2q",
    "outputId": "9b169bb7-b283-46bb-df19-672dffe4362a"
   },
   "outputs": [],
   "source": [
    "removeApos(df.loc[264,'tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv0ovHzLQ5Nw"
   },
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xvtmhi-0TKVK"
   },
   "source": [
    "**Remove one letter words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxzpbd_ETP7-"
   },
   "outputs": [],
   "source": [
    "def removeLetter(text):\n",
    "    text = text.split()\n",
    "    text = [word for word in text if  len(word) > 2]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w0yzUVTAK2S"
   },
   "source": [
    "**Remove stopWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25i_Ef2-AK2T",
    "outputId": "f55facd6-ef72-4a12-ee3e-12ba92cfb5f5"
   },
   "outputs": [],
   "source": [
    "nk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pw_iCgsrNcB7",
    "outputId": "70354f92-9e4d-4e18-e365-4d3943b47918"
   },
   "outputs": [],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uF2Jpr9YAK2T"
   },
   "outputs": [],
   "source": [
    "def removeStopWords(text):\n",
    "    sw = stopwords.words(\"english\")\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in sw]\n",
    "    return ' '.join(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "SjkwLOvwAK2U",
    "outputId": "bcf9a2bf-1703-41a6-e0cc-5906888ea87c"
   },
   "outputs": [],
   "source": [
    "df.loc[51].tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "pr4WOn-QAK2U",
    "outputId": "5ed62e0a-995b-406e-e58d-aab7e8b776fc"
   },
   "outputs": [],
   "source": [
    "removeStopWords(df.loc[51].tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nnkCqioLqoc"
   },
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwaxqgP5LDJk"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4qPZpuKL3C7"
   },
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    text = text.split()\n",
    "    text= [ps.stem(word) for word in text]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "1_dEi0HNMRWf",
    "outputId": "662b79a2-b002-4523-c128-8ce8f4b52b1a"
   },
   "outputs": [],
   "source": [
    "removeSpecialChar(df.loc[0,'tweet']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "gkQyLN5jMZwh",
    "outputId": "43ac64b2-f9ba-4299-fc23-422a0bf8377e"
   },
   "outputs": [],
   "source": [
    "stem(removeSpecialChar(df.loc[0,'tweet']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0wVxJJILGlU"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODA_kQ4GBh1z"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = lowerTweet(text)\n",
    "    text = removeURLs(text)\n",
    "    text = toAscii(text)\n",
    "    text = removeSpecialChar(text)\n",
    "    text = removeStopWords(text)\n",
    "    text = removeApos(text)\n",
    "    text = removeStopWords(text)\n",
    "    text = removeLetter(text)\n",
    "    text = stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "BbkXq8umBh10",
    "outputId": "9665e554-3007-4458-ee79-34c7c524b09d"
   },
   "outputs": [],
   "source": [
    "df.loc[209,'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MoYV-TKYBh11",
    "outputId": "feaa4fd6-9565-4b76-c1a2-515b40161656"
   },
   "outputs": [],
   "source": [
    "preprocess(df.loc[209,'tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZ6unuXQQQsf"
   },
   "source": [
    "# Tokenization (Not Necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oZ8nAmpAK2W"
   },
   "source": [
    "**Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_bcp7KTQAK2W",
    "outputId": "af2801a0-de2d-4dfa-909a-126adda7f9d1"
   },
   "outputs": [],
   "source": [
    "nk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Edze_HVqAK2W"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return nk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "27cbSYoQAK2X",
    "outputId": "f7c58d46-cb9c-45af-baff-7d8de055d5a5"
   },
   "outputs": [],
   "source": [
    "df.loc[51,'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkWFJCQHAK2X",
    "outputId": "f966830c-dec1-4d28-8599-757636279c3c"
   },
   "outputs": [],
   "source": [
    "tokenize(preprocess(df.loc[51,'tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkCjKE6rLYrU",
    "outputId": "02ffd232-387c-4cb1-e785-26d754bd1810"
   },
   "outputs": [],
   "source": [
    "tokenize('i ll see what would this give me')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrBhTTHKHivv"
   },
   "source": [
    "**Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz5P-tXXAK2X"
   },
   "outputs": [],
   "source": [
    "def bagOfWords(df):\n",
    "    tokens = []\n",
    "    bag = {}\n",
    "    for tweet in df.tweet:\n",
    "        tokens += tokenize(preprocess(tweet))\n",
    "    for token in tokens:\n",
    "        if token not in bag.keys():\n",
    "            bag[token] = 1\n",
    "        else:\n",
    "            bag[token] += 1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzSoYzC1AK2Y",
    "outputId": "5726b269-7a31-4602-f070-cef0981468b2"
   },
   "outputs": [],
   "source": [
    "bagOfWords(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrvUbllWdVu5"
   },
   "source": [
    "#  Weighting based on TF-IDF ( TfidfVectorizer ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zT12fiijOjwJ"
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "corpus = []\n",
    "for  tweet in df.tweet :\n",
    "  text = preprocess(tweet)\n",
    "  corpus.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2AmG0FoOj4b",
    "outputId": "04ded214-42b6-4c8c-8534-d897f871cfb2"
   },
   "outputs": [],
   "source": [
    "corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzHNfUjD0YVh"
   },
   "source": [
    "\n",
    "**Creating TF-IDF(Term Frequency and Inverse Document Frequency)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gXuHN9keZBq"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf=TfidfVectorizer().fit(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWdVVzNp0oVv"
   },
   "source": [
    "**Extracting all tokens i.e Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEB-i_XoOj7B",
    "outputId": "c70ca2e8-7803-4543-c02b-9115e3c12069"
   },
   "outputs": [],
   "source": [
    "count_tokens=tfidf.get_feature_names()\n",
    "count_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZoZEYRe1iOg"
   },
   "source": [
    "\n",
    "**Transforming the corpus into Vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9GvOg3-Oj-b",
    "outputId": "9fdfae9e-07d0-42c6-cd27-c00c3cdab685"
   },
   "outputs": [],
   "source": [
    "article_vect = tfidf.transform(corpus)\n",
    "article_vect.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2NdHwlW1xS7"
   },
   "source": [
    "**Displaying the final Converted Vector with Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "C8ag1SWS1-F6",
    "outputId": "5a0de088-4af1-4c07-d1a2-412972fcea40"
   },
   "outputs": [],
   "source": [
    "df_tfidf_vect=pd.DataFrame(data=article_vect.toarray(),columns=count_tokens)\n",
    "df_tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPAxHZ8UDg8z"
   },
   "outputs": [],
   "source": [
    "def TfIdfVec(df):\n",
    "    corpus = []\n",
    "    for  tweet in df.tweet :\n",
    "      text = preprocess(tweet)\n",
    "      corpus.append(text)\n",
    "\n",
    "    tfidf = TfidfVectorizer().fit(corpus)\n",
    "\n",
    "    tfidf_tokens = tfidf.get_feature_names()\n",
    "    article_vect = tfidf.transform(corpus)\n",
    "\n",
    "    df_tfidf_vect = pd.DataFrame(data=article_vect.toarray(),columns=tfidf_tokens)\n",
    "\n",
    "    return df_tfidf_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-M8-owV7DGa"
   },
   "source": [
    "# Weighting based on TF ( CountVectorizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVVVMF9B7wMu"
   },
   "outputs": [],
   "source": [
    "#https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a\n",
    "corpus = []\n",
    "for  tweet in df.tweet :\n",
    "  text = preprocess(tweet)\n",
    "  corpus.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MBtLKQs7wMv",
    "outputId": "a11a69a5-7088-4e9f-fceb-2ea3154182fa"
   },
   "outputs": [],
   "source": [
    "corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKhC0B-x7wMv"
   },
   "source": [
    "\n",
    "**Creating TF-IDF(Term Frequency and Inverse Document Frequency)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6IhJpLC7wMv"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count=CountVectorizer().fit(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIuBE2qo7wMv"
   },
   "source": [
    "**Extracting all tokens i.e Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYCKsdRi7wMv",
    "outputId": "8d495b3f-b395-40fd-bf13-5d116dbd760f"
   },
   "outputs": [],
   "source": [
    "count_tokens=count.get_feature_names()\n",
    "count_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m80qNjEu7wMw"
   },
   "source": [
    "\n",
    "**Transforming the corpus into Vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TS3_u0RG7wMw",
    "outputId": "3173cc2f-c280-4952-cd02-cef09ce40faa"
   },
   "outputs": [],
   "source": [
    "article_vect = count.transform(corpus)\n",
    "article_vect.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYod60sr7wMw"
   },
   "source": [
    "**Displaying the final Converted Vector with Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "E6Wu9vr-7wMw",
    "outputId": "745426a3-cf20-417f-9e7a-889adb32a435"
   },
   "outputs": [],
   "source": [
    "df_count_vect=pd.DataFrame(data=article_vect.toarray(),columns=count_tokens)\n",
    "df_count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KSzppbbCytQ"
   },
   "outputs": [],
   "source": [
    "def CountVec(df):\n",
    "    corpus = []\n",
    "    for  tweet in df.tweet :\n",
    "      text = preprocess(tweet)\n",
    "      corpus.append(text)\n",
    "\n",
    "    count = CountVectorizer().fit(corpus)\n",
    "\n",
    "    count_tokens = count.get_feature_names()\n",
    "    article_vect = count.transform(corpus)\n",
    "\n",
    "    df_count_vect = pd.DataFrame(data=article_vect.toarray(),columns=count_tokens)\n",
    "\n",
    "    return df_count_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53jNPIie9Ytx"
   },
   "source": [
    "# Dimensionality Reduction based on Frequency Count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZM4wQpbMbyn"
   },
   "source": [
    "The goal is to reduce the dimension of the vectorized data by removing columns (features) based on their document frequency in other words if   the values of a specific column are not different from 0 in more than a specific threshold(seuil) we remove the column from the DateFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KREzfM8NMqNE"
   },
   "source": [
    "**Vectorizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aK3sW83XMU1h"
   },
   "outputs": [],
   "source": [
    "dfVec = TfIdfVec(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "KfcfBmABEuV1",
    "outputId": "7ae3b7c4-42b0-41c5-c1b9-4aba8a89dbe6"
   },
   "outputs": [],
   "source": [
    "dfVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ippWyCKSM0EI"
   },
   "source": [
    "**Selecting the list of columns that do not acheive a specific threshold(seuil)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "Rf9QwOPeFvMc",
    "outputId": "d813900e-6c5c-4e88-b6dc-8dfe7880ce3c"
   },
   "outputs": [],
   "source": [
    "subDf = dfVec[dfVec != 0]\n",
    "subDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajcLjlx2GtjQ",
    "outputId": "79276270-5ea5-4412-f1c2-df98ba6aa705"
   },
   "outputs": [],
   "source": [
    "subDf.count(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpSdvPWeG5HL",
    "outputId": "6bda9694-8d24-409d-b200-79598c416aea"
   },
   "outputs": [],
   "source": [
    "map = subDf.count(axis = 0) <= 5\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZDIfXKwNBt-"
   },
   "source": [
    "**Droping the columns selected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "A6kum1y6HeRR",
    "outputId": "a06efb71-bdf5-4d5f-bcb1-81b8175b21e7"
   },
   "outputs": [],
   "source": [
    "dfVec.drop(dfVec.columns[map],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H143Zz00J0vU"
   },
   "outputs": [],
   "source": [
    "def redDim(dfVec,seuil):\n",
    "    subDf = dfVec[dfVec != 0]\n",
    "    map = subDf.count(axis = 0) <= seuil\n",
    "    resultDf = dfVec.drop(dfVec.columns[map],axis=1)\n",
    "\n",
    "    return resultDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "LMKhqUT_PU0f",
    "svD7uZt6PgxM",
    "Pv0ovHzLQ5Nw",
    "P0wVxJJILGlU",
    "bZ6unuXQQQsf",
    "RrvUbllWdVu5",
    "a-M8-owV7DGa"
   ],
   "name": "Projet_IA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
