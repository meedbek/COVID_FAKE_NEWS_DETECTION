{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FZ3uPDa-sENJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Covid_Dataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        super(Covid_Dataset, self)\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Qe3HFD33sEYm"
   },
   "outputs": [],
   "source": [
    "#load the original data and the new prepared data\n",
    "df = pd.read_excel(\"../CSV/Data-FakeRealCOVID.xlsx\")\n",
    "newDF = pd.read_csv(\"../CSV/newDF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lBhZ6rdYsiq3"
   },
   "outputs": [],
   "source": [
    "#drop a column that contains indexs (it is created automatically when saving a dataFrame to a csv file)\n",
    "newDF = newDF.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pourcentage of fake and real news \n",
    "df.label.hist(xlabelsize=20,figsize=[5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "wOV71dtes3kg",
    "outputId": "8ba24130-679a-4e54-e35a-3540a77ce486"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aadajoli</th>\n",
       "      <th>aai</th>\n",
       "      <th>aaj</th>\n",
       "      <th>aajtak</th>\n",
       "      <th>aamaadmiparti</th>\n",
       "      <th>aamctoday</th>\n",
       "      <th>aamir</th>\n",
       "      <th>aaradhya</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoodirector</th>\n",
       "      <th>zookeep</th>\n",
       "      <th>zoolog</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zubymus</th>\n",
       "      <th>zydu</th>\n",
       "      <th>zyphr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6415</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6416</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6417</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6418</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.419268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6419</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6420 rows × 9065 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aadajoli  aai  aaj  aajtak  aamaadmiparti  aamctoday     aamir  \\\n",
       "0     0.0       0.0  0.0  0.0     0.0            0.0        0.0  0.000000   \n",
       "1     0.0       0.0  0.0  0.0     0.0            0.0        0.0  0.000000   \n",
       "2     0.0       0.0  0.0  0.0     0.0            0.0        0.0  0.000000   \n",
       "3     0.0       0.0  0.0  0.0     0.0            0.0        0.0  0.000000   \n",
       "4     0.0       0.0  0.0  0.0     0.0            0.0        0.0  0.000000   \n",
       "...   ...       ...  ...  ...     ...            ...        ...       ...   \n",
       "6415  0.0       0.0  0.0  0.0     0.0            0.0        0.0  0.000000   \n",
       "6416  0.0       0.0  0.0  0.0     0.0            0.0        0.0  0.000000   \n",
       "6417  0.0       0.0  0.0  0.0     0.0            0.0        0.0  0.000000   \n",
       "6418  0.0       0.0  0.0  0.0     0.0            0.0        0.0  0.419268   \n",
       "6419  0.0       0.0  0.0  0.0     0.0            0.0        0.0  0.000000   \n",
       "\n",
       "      aaradhya   ab  ...  zombi  zone  zoo  zoodirector  zookeep  zoolog  \\\n",
       "0          0.0  0.0  ...    0.0   0.0  0.0          0.0      0.0     0.0   \n",
       "1          0.0  0.0  ...    0.0   0.0  0.0          0.0      0.0     0.0   \n",
       "2          0.0  0.0  ...    0.0   0.0  0.0          0.0      0.0     0.0   \n",
       "3          0.0  0.0  ...    0.0   0.0  0.0          0.0      0.0     0.0   \n",
       "4          0.0  0.0  ...    0.0   0.0  0.0          0.0      0.0     0.0   \n",
       "...        ...  ...  ...    ...   ...  ...          ...      ...     ...   \n",
       "6415       0.0  0.0  ...    0.0   0.0  0.0          0.0      0.0     0.0   \n",
       "6416       0.0  0.0  ...    0.0   0.0  0.0          0.0      0.0     0.0   \n",
       "6417       0.0  0.0  ...    0.0   0.0  0.0          0.0      0.0     0.0   \n",
       "6418       0.0  0.0  ...    0.0   0.0  0.0          0.0      0.0     0.0   \n",
       "6419       0.0  0.0  ...    0.0   0.0  0.0          0.0      0.0     0.0   \n",
       "\n",
       "      zoom  zubymus  zydu  zyphr  \n",
       "0      0.0      0.0   0.0    0.0  \n",
       "1      0.0      0.0   0.0    0.0  \n",
       "2      0.0      0.0   0.0    0.0  \n",
       "3      0.0      0.0   0.0    0.0  \n",
       "4      0.0      0.0   0.0    0.0  \n",
       "...    ...      ...   ...    ...  \n",
       "6415   0.0      0.0   0.0    0.0  \n",
       "6416   0.0      0.0   0.0    0.0  \n",
       "6417   0.0      0.0   0.0    0.0  \n",
       "6418   0.0      0.0   0.0    0.0  \n",
       "6419   0.0      0.0   0.0    0.0  \n",
       "\n",
       "[6420 rows x 9065 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we already defined this function on Vecotrization file\n",
    "def redDim(dfVec,seuil1,seuil2):\n",
    "    subDf = dfVec[dfVec != 0]\n",
    "    map = subDf.count(axis = 0) <= seuil1 \n",
    "    map2 = subDf.count(axis = 0) >= (dfVec.shape[0] - seuil2) \n",
    "    resultDf = dfVec.drop(dfVec.columns[map+map2],axis=1)\n",
    "    columns_dropped=dfVec.columns[map+map2]\n",
    "    \n",
    "    return resultDf,columns_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KzM25Jlb61y9",
    "outputId": "e918e3de-3d8d-4df6-f795-2540e0dfa482"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a tensor containing targets(fake,real)\n",
    "labels = []\n",
    "for label in df.label.to_numpy():\n",
    "  if label == 'real':\n",
    "    labels.append(1)\n",
    "  else:\n",
    "    labels.append(0)\n",
    "targets=torch.tensor(labels)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWC7l7AlrRvY",
    "outputId": "0e17615b-bd57-4ef9-9464-487849f545cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_cRVsJ5P61wJ"
   },
   "outputs": [],
   "source": [
    "data=torch.tensor(newDF.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFo0D2UDrL9f",
    "outputId": "4dd9c4d3-3044-4df3-f7b7-80ac8ce04d7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_LqR9Z4fsEbQ"
   },
   "outputs": [],
   "source": [
    "# Importer la fonction de splitting des données de scikit learn\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9FvJq-RJxxX",
    "outputId": "db9716d2-df2b-41b8-a902-529ce0debb9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5136, 9065])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer 4 tensors en résultat du splitting: données d'apprentissage, données de test, labels d'apprentissage, et labels de test  \n",
    "atrain_data, test_data, atrain_targets, test_targets = train_test_split(data, targets, test_size=0.2)\n",
    "atrain_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avlRzUHBsEd0",
    "outputId": "c28991ec-d8a0-4919-94d8-4e87fb9c73ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4136, 9065])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer 4 tensors en résultat du splitting: données d'apprentissage, données de validation, labels d'apprentissage, et labels de validation  \n",
    "train_data, validation_data, train_targets, validation_targets = train_test_split(atrain_data, atrain_targets, test_size=1000)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7D9yvLbsEjV",
    "outputId": "3eab84ea-06aa-4025-9aa5-b66a43a35551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4136, 9065]) torch.Size([4136])\n",
      "torch.Size([1000, 9065]) torch.Size([1000])\n",
      "torch.Size([1284, 9065]) torch.Size([1284])\n"
     ]
    }
   ],
   "source": [
    "# Afficher la taille de chaque tensor\n",
    "print(train_data.shape, train_targets.shape)\n",
    "print(validation_data.shape, validation_targets.shape)\n",
    "print(test_data.shape,test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CoS4SEElK18d"
   },
   "outputs": [],
   "source": [
    "# Importer Dataset de torch.utils.data\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "p00R4rbYLHy6"
   },
   "outputs": [],
   "source": [
    "# Créer une classe qui hérite de Dataset et redéfinit les méthodes \n",
    "class Covid_Dataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        super(Covid_Dataset, self)\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "n5NcCk_gLR0P"
   },
   "outputs": [],
   "source": [
    "# Créer les 3 objets en instantiant votre classe\n",
    "train_dataset = Covid_Dataset(train_data, train_targets)\n",
    "test_dataset = Covid_Dataset(test_data, test_targets)\n",
    "validation_dataset = Covid_Dataset(validation_data, validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZnltqtTlLlSP"
   },
   "outputs": [],
   "source": [
    "# Importer DataLoader de torch.utils.data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IIParFTvLn8i"
   },
   "outputs": [],
   "source": [
    "# Créer une variable pour la taille du batch\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NiBGAcYgL5GV"
   },
   "outputs": [],
   "source": [
    "# Créer les objets DataLoader pour vos datasets d'apprentissage, test et validation en lui donner la taille du batch convenue\n",
    "\n",
    "train_DL = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_DL = DataLoader(test_dataset, batch_size=batch_size)\n",
    "validation_DL = DataLoader(validation_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wzmpmmyEf_pt",
    "outputId": "e2bc1ab7-c5cb-4836-fcc9-2556fb26445d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7yQIn6kpgGtG"
   },
   "outputs": [],
   "source": [
    "# Importer le module nn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9065"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZdLwPkOSj20C"
   },
   "outputs": [],
   "source": [
    "# En utilisant Sequential(), créer un modèle avec l'architecture susmentionnée\n",
    "CovidNN = nn.Sequential(nn.Linear(train_data.shape[1], 64),nn.ReLU(),nn.Linear(64, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nKcOOgT6mLLV"
   },
   "outputs": [],
   "source": [
    "# Définir la fonction du coût. On peut choisir CrossEntropyLoss\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2eK8iWgXm1NK"
   },
   "outputs": [],
   "source": [
    "# Définir une fonction d'optimisation des coût: Adam par exemple. On devra définir un learning rate. On choisira 0.001.\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(CovidNN.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "2NrocaPSpPet",
    "outputId": "8e4cd098-8f2f-418b-d3a6-800b197605d3"
   },
   "outputs": [],
   "source": [
    "#cette fonction permet de faire le training\n",
    "def training(model,train_DL,validation_DL,nb_epochs,optimizer,loss_function):\n",
    "    # Créer une boucle sur les epochs:\n",
    "    for i in range(nb_epochs):\n",
    "        # Spécifier qu'on est sur le mode entraînement\n",
    "        model.train()\n",
    "        # initialiser notre coût d'apprentissage à 0.0\n",
    "        cout_appr = 0\n",
    "        # Boucler sur les minibatchs des données d'entaînement (les données et leurs targets):\n",
    "        for data,targets in train_DL:    \n",
    "            # le vecteur des labels prédites par le modèle est le résultat de l'application du modèle sur le minibatch en cours. \n",
    "            output = model(data.float())\n",
    "            # Calculer le coût en comparant les labels prédits aux targets du minibatch\n",
    "            loss = loss_function(output,targets)\n",
    "            # Backpropagation: \n",
    "            # Réinitialiser l'optimiseur\n",
    "            optimizer.zero_grad()\n",
    "            # Faire la backpropagation\n",
    "            loss.backward()\n",
    "            # Effectuer un pas d'optimisation\n",
    "            optimizer.step()\n",
    "            # Mettre à jour votre coût d'apprentissage en lui ajoutant le coût du data batch\n",
    "            cout_appr += loss.item()\n",
    "        # A la sortie de la boucle de l'entraînement, on calcule le coût moyen pour toutes les données training\n",
    "        cout_moyen = cout_appr/len(train_DL.dataset)\n",
    "        print('Train_Loss:',cout_moyen)\n",
    "        # Initiliser le coût de validation à 0.0\n",
    "        cout_valid = 0\n",
    "        # Initialiser le nombre de prévisions correctes à 0\n",
    "        prevision_correcte = 0\n",
    "        # Spécifier qu'on est sur le mode d'évaluation\n",
    "        model.eval()\n",
    "        # Indiquer à Pytorch qu'on ne va pas faire de Gradient descent (comme on est dans l'évaluation)\n",
    "        with torch.no_grad():\n",
    "            # Boucler sur les minibatchs des données de validation (les données et leurs targets):\n",
    "            for data,targets in validation_DL:  \n",
    "                # le vecteur des labels prédites par le modèle est le résultat de l'application du modèle sur le minibatch en cours. \n",
    "                output = model(data.float())\n",
    "                # Calculer le coût en comparant les labels prédits aux targets du minibatch\n",
    "                loss = loss_function(output,targets)\n",
    "                # Mettre à jour votre coût de validation en lui ajoutant le coût du data batch\n",
    "                cout_valid += loss.item()\n",
    "                # Mettre à jour le nombre de prévision correctes en y ajoutant le nombre des bonnes prévision sur ce batch\n",
    "                # On y compare le label prédit avec le labels du minibatch. \n",
    "                # Penser à utiliser argmax pour avoir la prévision finale à partir du vecteur de prévision\n",
    "                #prevision_correcte += len(output[output == targets])\n",
    "                prevision_correcte += torch.sum((torch.argmax(output, dim=1) == targets)).item()\n",
    "            # A la sortie de cette boucle, calculer le coût moyen de validation\n",
    "            cout_valid_moyenne = cout_valid/len(validation_DL.dataset)\n",
    "            print('Valid_Loss:',cout_valid_moyenne)\n",
    "            # Calculer la précision: la moyenne des prévisions correctes sur l'ensemble des observations dans le dataset validation \n",
    "            prevision_correcte_moyenne = prevision_correcte/len(validation_DL.dataset)\n",
    "            print('Accuracy:',prevision_correcte_moyenne*100,'%\\n')\n",
    "            \n",
    "        # Afficher pour chaque itération le coût d'entraînement, le coût de validation, et la précision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reinstialiser le model\n",
    "for layer in CovidNN.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Loss: 0.010824366409612578\n",
      "Valid_Loss: 0.010956307470798492\n",
      "Accuracy: 50.1 %\n",
      "\n",
      "Train_Loss: 0.010586309940256972\n",
      "Valid_Loss: 0.010658431529998779\n",
      "Accuracy: 71.3 %\n",
      "\n",
      "Train_Loss: 0.010181612445030729\n",
      "Valid_Loss: 0.010249186694622039\n",
      "Accuracy: 84.39999999999999 %\n",
      "\n",
      "Train_Loss: 0.009674891915957749\n",
      "Valid_Loss: 0.009775661587715149\n",
      "Accuracy: 88.0 %\n",
      "\n",
      "Train_Loss: 0.009104152280311289\n",
      "Valid_Loss: 0.009259840369224549\n",
      "Accuracy: 89.5 %\n",
      "\n",
      "Train_Loss: 0.008496717065284515\n",
      "Valid_Loss: 0.008725327730178834\n",
      "Accuracy: 90.0 %\n",
      "\n",
      "Train_Loss: 0.007879089275861386\n",
      "Valid_Loss: 0.00819373095035553\n",
      "Accuracy: 90.4 %\n",
      "\n",
      "Train_Loss: 0.007273779695626615\n",
      "Valid_Loss: 0.007682411551475525\n",
      "Accuracy: 90.60000000000001 %\n",
      "\n",
      "Train_Loss: 0.006697559271663485\n",
      "Valid_Loss: 0.007203244000673294\n",
      "Accuracy: 90.5 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tester la fonction training avec un nombre d'epochs 9\n",
    "training(CovidNN,train_DL,validation_DL,9,optimizer,loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction qui permet de test le model\n",
    "def testModel(model,test_DL,loss_function):\n",
    "    # Initiliser le coût de test à 0.0\n",
    "    cout_test = 0\n",
    "    # Initialiser le nombre de prévisions correctes à 0\n",
    "    prevision_correcte = 0\n",
    "    with torch.no_grad():\n",
    "        # Boucler sur les minibatchs des données de test (les données et leurs targets):\n",
    "        for data,targets in test_DL:  \n",
    "            # le vecteur des labels prédites par le modèle est le résultat de l'application du modèle sur le minibatch en cours. \n",
    "            output = model(data.float())\n",
    "            # Calculer le coût en comparant les labels prédits aux targets du minibatch\n",
    "            loss = loss_function(output,targets)\n",
    "            # Mettre à jour votre coût de test en lui ajoutant le coût du data batch\n",
    "            cout_test += loss.item()\n",
    "            # Mettre à jour le nombre de prévision correctes en y ajoutant le nombre des bonnes prévision sur ce batch\n",
    "            # On y compare le label prédit avec le labels du minibatch. \n",
    "            # Penser à utiliser argmax pour avoir la prévision finale à partir du vecteur de prévision\n",
    "            #prevision_correcte += len(output[output == targets])\n",
    "            prevision_correcte += torch.sum((torch.argmax(output, dim=1) == targets)).item()\n",
    "        # A la sortie de cette boucle, calculer le coût moyen de test\n",
    "        cout_test_moyenne = cout_test/len(test_DL.dataset)\n",
    "\n",
    "        # Calculer la précision: la moyenne des prévisions correctes sur l'ensemble des observations dans le dataset test \n",
    "        prevision_correcte_moyenne = prevision_correcte/len(test_DL.dataset)\n",
    "        print(\"Testing results :\\nLoss : \",cout_test_moyenne,\"\\nAccuracy : \",prevision_correcte_moyenne*100,\"%\\n\")\n",
    "        \n",
    "        return cout_test_moyenne,prevision_correcte_moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results :\n",
      "Loss :  0.007232914577206347 \n",
      "Accuracy :  91.1214953271028 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.007232914577206347, 0.9112149532710281)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testModel(CovidNN,test_DL,loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction qui permet de creer un model et faire le test avec differents hyperparametres en entré et retroune le model + les columns qui ont été supprimer\n",
    "#du dataFrame avant d'entamer la phase d'apprentissage\n",
    "def createModel(dataDF,targets,batch_size,neuronesNb,learning_rate,epochs,activationFunction,seuil1=0,seuil2=0):\n",
    "    \n",
    "    newDF,columnsToDrop = redDim(dataDF,seuil1,seuil2)\n",
    "    data=torch.tensor(newDF.to_numpy())\n",
    "\n",
    "\n",
    "    # Créer 4 tensors en résultat du splitting: données d'apprentissage, données de test, labels d'apprentissage, et labels de test  \n",
    "    atrain_data, test_data, atrain_targets, test_targets = train_test_split(data, targets, test_size=0.2)\n",
    "\n",
    "    # Créer 4 tensors en résultat du splitting: données d'apprentissage, données de validation, labels d'apprentissage, et labels de validation  \n",
    "    train_data, validation_data, train_targets, validation_targets = train_test_split(atrain_data, atrain_targets, test_size=1000)\n",
    "\n",
    "    # Créer les 3 objets en instantiant votre classe\n",
    "    train_dataset = Covid_Dataset(train_data, train_targets)\n",
    "    test_dataset = Covid_Dataset(test_data, test_targets)\n",
    "    validation_dataset = Covid_Dataset(validation_data, validation_targets)\n",
    "\n",
    "\n",
    "    # Créer les objets DataLoader pour vos datasets d'apprentissage, test et validation en lui donner la taille du batch convenue\n",
    "\n",
    "    train_DL = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    test_DL = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    validation_DL = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "\n",
    "    # En utilisant Sequential(), créer un modèle avec l'architecture susmentionnée\n",
    "    CovidNN = nn.Sequential(nn.Linear(train_data.shape[1], neuronesNb),activationFunction,nn.Linear(neuronesNb, 2))\n",
    "\n",
    "    # Définir la fonction du coût. On peut choisir CrossEntropyLoss\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(CovidNN.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    for layer in CovidNN.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    training(CovidNN,train_DL,validation_DL,epochs,optimizer,loss_function)\n",
    "\n",
    "    loss,accuracy = testModel(CovidNN,test_DL,loss_function)\n",
    "    \n",
    "    return CovidNN,columnsToDrop,loss,accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create some models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le prototype de la fonction est le suivant :\n",
    "```createModel(dataDF,targets,batch_size,neuronesNb,learning_rate,epochs,activationFunction,seuil1=0,seuil2=0)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Loss: 0.010860481088696411\n",
      "Valid_Loss: 0.010988358795642852\n",
      "Accuracy: 59.099999999999994 %\n",
      "\n",
      "Train_Loss: 0.010648060064020654\n",
      "Valid_Loss: 0.010720562279224397\n",
      "Accuracy: 88.0 %\n",
      "\n",
      "Train_Loss: 0.010271389621023514\n",
      "Valid_Loss: 0.010330653488636017\n",
      "Accuracy: 90.60000000000001 %\n",
      "\n",
      "Train_Loss: 0.009772877297157015\n",
      "Valid_Loss: 0.00986653208732605\n",
      "Accuracy: 90.0 %\n",
      "\n",
      "Train_Loss: 0.00920240692001708\n",
      "Valid_Loss: 0.009358839333057404\n",
      "Accuracy: 90.60000000000001 %\n",
      "\n",
      "Train_Loss: 0.008591391146183014\n",
      "Valid_Loss: 0.00883143788576126\n",
      "Accuracy: 90.60000000000001 %\n",
      "\n",
      "Train_Loss: 0.007966995246463403\n",
      "Valid_Loss: 0.00830615884065628\n",
      "Accuracy: 90.7 %\n",
      "\n",
      "Train_Loss: 0.007352776457942893\n",
      "Valid_Loss: 0.007800271213054657\n",
      "Accuracy: 90.7 %\n",
      "\n",
      "Testing results :\n",
      "Loss :  0.007783376748131072 \n",
      "Accuracy :  89.95327102803739 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pour le learning rate lr = 0.0001\n",
    "model1 = createModel(newDF,targets,64,64,0.0001,8,nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le model 1 est plus au moins bon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Loss: 0.010425998939190427\n",
      "Valid_Loss: 0.009714247941970825\n",
      "Accuracy: 88.9 %\n",
      "\n",
      "Train_Loss: 0.008120431415232283\n",
      "Valid_Loss: 0.00707448348402977\n",
      "Accuracy: 89.8 %\n",
      "\n",
      "Train_Loss: 0.00544690812723549\n",
      "Valid_Loss: 0.005145280450582504\n",
      "Accuracy: 90.60000000000001 %\n",
      "\n",
      "Train_Loss: 0.003708686034701318\n",
      "Valid_Loss: 0.004129650741815567\n",
      "Accuracy: 91.60000000000001 %\n",
      "\n",
      "Train_Loss: 0.0027069425520197103\n",
      "Valid_Loss: 0.0035904650986194613\n",
      "Accuracy: 92.10000000000001 %\n",
      "\n",
      "Train_Loss: 0.0020808097699249965\n",
      "Valid_Loss: 0.0032753301858901977\n",
      "Accuracy: 92.60000000000001 %\n",
      "\n",
      "Train_Loss: 0.0016523799219491174\n",
      "Valid_Loss: 0.00307673180103302\n",
      "Accuracy: 92.9 %\n",
      "\n",
      "Train_Loss: 0.0013408263473333774\n",
      "Valid_Loss: 0.0029454570189118385\n",
      "Accuracy: 93.2 %\n",
      "\n",
      "Testing results :\n",
      "Loss :  0.00315672178923898 \n",
      "Accuracy :  92.36760124610592 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pour un lr = 0.001\n",
    "model2 = createModel(newDF,targets,64,64,0.0005,8,nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est tombé sur l'overfitting il faut diminuer le nombre d'epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Loss: 0.010401310020881188\n",
      "Valid_Loss: 0.009685563206672668\n",
      "Accuracy: 82.0 %\n",
      "\n",
      "Train_Loss: 0.008062282602376587\n",
      "Valid_Loss: 0.007125015079975128\n",
      "Accuracy: 90.9 %\n",
      "\n",
      "Train_Loss: 0.005389854722661714\n",
      "Valid_Loss: 0.005253640949726105\n",
      "Accuracy: 92.10000000000001 %\n",
      "\n",
      "Train_Loss: 0.0036995999255655353\n",
      "Valid_Loss: 0.004261264503002167\n",
      "Accuracy: 92.4 %\n",
      "\n",
      "Train_Loss: 0.0027314709495875546\n",
      "Valid_Loss: 0.0037172036319971084\n",
      "Accuracy: 92.80000000000001 %\n",
      "\n",
      "Testing results :\n",
      "Loss :  0.0035001589130389727 \n",
      "Accuracy :  93.45794392523365 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pour un lr = 0.001\n",
    "model3 = createModel(newDF,targets,64,64,0.0005,5,nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modèle mieux du premier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Loss: 0.010401225470482035\n",
      "Valid_Loss: 0.009657237648963929\n",
      "Accuracy: 65.8 %\n",
      "\n",
      "Train_Loss: 0.008146777806706309\n",
      "Valid_Loss: 0.007059908598661423\n",
      "Accuracy: 90.2 %\n",
      "\n",
      "Train_Loss: 0.005583490549480431\n",
      "Valid_Loss: 0.0051777146756649016\n",
      "Accuracy: 91.3 %\n",
      "\n",
      "Train_Loss: 0.003954386850188503\n",
      "Valid_Loss: 0.004196228444576264\n",
      "Accuracy: 91.60000000000001 %\n",
      "\n",
      "Train_Loss: 0.0030270332823610167\n",
      "Valid_Loss: 0.0036744936257600783\n",
      "Accuracy: 91.9 %\n",
      "\n",
      "Testing results :\n",
      "Loss :  0.0041925692669699125 \n",
      "Accuracy :  89.797507788162 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#on ajoute des seuils pour diminuer le nombre de features\n",
    "model4 = createModel(newDF,targets,64,64,0.0005,5,nn.ReLU(),seuil1=2,seuil2=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le model precedent est mieu de ceci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Loss: 0.005788620979285563\n",
      "Valid_Loss: 0.00332843828946352\n",
      "Accuracy: 91.10000000000001 %\n",
      "\n",
      "Train_Loss: 0.0014307754018796136\n",
      "Valid_Loss: 0.0032679019197821616\n",
      "Accuracy: 91.0 %\n",
      "\n",
      "Train_Loss: 0.0005471902495030335\n",
      "Valid_Loss: 0.0034680057987570762\n",
      "Accuracy: 91.8 %\n",
      "\n",
      "Train_Loss: 0.00024924607444428467\n",
      "Valid_Loss: 0.003786338895559311\n",
      "Accuracy: 91.7 %\n",
      "\n",
      "Train_Loss: 0.00013007341592025004\n",
      "Valid_Loss: 0.004095023229718209\n",
      "Accuracy: 91.3 %\n",
      "\n",
      "Testing results :\n",
      "Loss :  0.0036004767800330234 \n",
      "Accuracy :  91.43302180685359 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tester avec seuil1\n",
    "model6 = createModel(newDF,targets,64,64,0.005,5,nn.ReLU(),seuil1=1,seuil2=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Loss: 0.010403132891170752\n",
      "Valid_Loss: 0.009777044832706451\n",
      "Accuracy: 76.0 %\n",
      "\n",
      "Train_Loss: 0.008275993773623638\n",
      "Valid_Loss: 0.00731196990609169\n",
      "Accuracy: 88.3 %\n",
      "\n",
      "Train_Loss: 0.005863778228282468\n",
      "Valid_Loss: 0.005456398159265518\n",
      "Accuracy: 90.4 %\n",
      "\n",
      "Train_Loss: 0.004287202984844001\n",
      "Valid_Loss: 0.004446498334407806\n",
      "Accuracy: 90.7 %\n",
      "\n",
      "Train_Loss: 0.00337377595544092\n",
      "Valid_Loss: 0.0038928172439336777\n",
      "Accuracy: 91.2 %\n",
      "\n",
      "Testing results :\n",
      "Loss :  0.0040503349851503554 \n",
      "Accuracy :  90.73208722741433 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#augmenter les seuils\n",
    "model5 = createModel(newDF,targets,64,64,0.0005,5,nn.ReLU(),seuil1=5,seuil2=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut conclure que c'est mieux de ne pas inclure les seuils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder le modèle dans un fichier pickle\n",
    "#modelFileName='CovidNN.tkl'\n",
    "#torch.save(CovidNN,modelFileName)\n",
    "#torch.save(CovidNN,'interface_graphique/'+modelFileName)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NN_MODEL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
